{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063e96a4-b2c9-4265-9798-feb042d0dcc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import logging\n",
    "\n",
    "# Iniciar a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ler Parquet e Escrever no PostgreSQL\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do Logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Criar um logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Criando sessão Spark....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d268df82-faa5-411b-a590-dee56aaf2825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trimestre: string (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- tipo: string (nullable = true)\n",
      " |-- cnpj_if: string (nullable = true)\n",
      " |-- instituicao_financeira: string (nullable = true)\n",
      " |-- indice: string (nullable = true)\n",
      " |-- quantidade_de_reclamacoes_reguladas_procedentes: string (nullable = true)\n",
      " |-- quantidade_de_reclamacoes_reguladas_outras: string (nullable = true)\n",
      " |-- quantidade_de_reclamacoes_nao_reguladas: string (nullable = true)\n",
      " |-- quantidade_total_de_reclamacoes: string (nullable = true)\n",
      " |-- quantidade_total_de_clientes_ccs_e_scr: string (nullable = true)\n",
      " |-- quantidade_de_cliente_ccs: string (nullable = true)\n",
      " |-- quantidade_de_cliente_scr: string (nullable = true)\n",
      " |-- employer_website: string (nullable = true)\n",
      " |-- employer_headquarters: string (nullable = true)\n",
      " |-- employer_founded: string (nullable = true)\n",
      " |-- employer_industry: string (nullable = true)\n",
      " |-- employer_revenue: string (nullable = true)\n",
      " |-- geral: string (nullable = true)\n",
      " |-- cultura_e_valores: string (nullable = true)\n",
      " |-- diversidade_e_inclusao: string (nullable = true)\n",
      " |-- qualidade_de_vida: string (nullable = true)\n",
      " |-- alta_lideranca: string (nullable = true)\n",
      " |-- remuneracao_e_beneficios: string (nullable = true)\n",
      " |-- oportunidades_de_carreira: string (nullable = true)\n",
      " |-- recomendam_para_outras_pessoas: string (nullable = true)\n",
      " |-- perspectiva_positiva_da_empresa: string (nullable = true)\n",
      " |-- cnpj: string (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- segmento: string (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Reading Postgres Authentication Params. Please wait.\")\n",
    "# Ler o arquivo Parquet\n",
    "df_unifiedData_gold = spark.read.parquet(\"/FileStore/glassdoor/gold/glassdoor_tb\")\n",
    "try: \n",
    "    url = \"https://sqliteonline.com/#sharedb=p%3Abig\"\n",
    "    table = \"public.glassdoor_grupo1_atividade3\"\n",
    "    user = \"postgres\"\n",
    "    password = \"root\"\n",
    "\n",
    "    df_unifiedData_gold.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error!! Não foi possível ler parametros de autenticação do Postgres: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e1637f-6e18-43de-b231-f6135287f219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao salvar os dados: An error occurred while calling o1811.jdbc.\n",
      ": org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections.\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:319)\n",
      "\tat org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)\n",
      "\tat org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:223)\n",
      "\tat org.postgresql.Driver.makeConnection(Driver.java:400)\n",
      "\tat org.postgresql.Driver.connect(Driver.java:259)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:123)\n",
      "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:119)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.saveTableToJDBC(JdbcRelationProvider.scala:52)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n",
      "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:861)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
      "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
      "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
      "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.net.Socket.connect(Socket.java:613)\n",
      "\tat org.postgresql.core.PGStream.createSocket(PGStream.java:241)\n",
      "\tat org.postgresql.core.PGStream.<init>(PGStream.java:98)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:109)\n",
      "\tat org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:235)\n",
      "\t... 61 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"PostgreSQL setup and properties. Please wait while processing...\")\n",
    "# Ler o arquivo Parquet\n",
    "df_unifiedData_gold = spark.read.parquet(\"/FileStore/glassdoor/gold/glassdoor_tb\")\n",
    "try: \n",
    "    # Configurar o ambiente Spark para usar o PostgreSQL\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Write DataFrame to PostgreSQL\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.5\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Corrigir a URL de conexão\n",
    "    url = \"jdbc:postgresql://localhost:5432/glassdoor_db\"\n",
    "\n",
    "    # Configurações para a conexão JDBC\n",
    "    properties = {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"root\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    logger.info(df_unifiedData_gold.printSchema())\n",
    "    logger.info(\"Configuração do PostgreSQL realizada com sucesso!\")\n",
    "    logger.info(\"Tentativa de realizar o import dos dados no PostgreSQL. Aguarde....\")\n",
    "    # Tentativa de salvar o DataFrame no PostgreSQL\n",
    "    try:\n",
    "        df_unifiedData_gold.write \\\n",
    "            .jdbc(url=url, table=\"public.glassdoor_grupo1_atividade3\", mode=\"overwrite\", properties=properties)\n",
    "        print(\"Dados salvos com sucesso no PostgreSQL.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Erro ao salvar os dados no PostgreSQL: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"ERROR - Não foi possível realizar configuração do PostgreSQL: {e}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LOAD_DF_TO_DB_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
